{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW5 Problem:  Predicting Solubility -- using Applied Predictive Modeling\n",
    "\n",
    "For this problem you will analyze a variant of the <i>Solubility</i> dataset studied in [APM]\n",
    "(the <i>Applied Predictive Modeling</i> course text).  You can use the results about Solubility in this book in any way you choose.\n",
    "\n",
    "This is easy if you use the APM methodology, which helps automate construction of models.\n",
    "Some code from the [APM] book is also included below.\n",
    "\n",
    "Because the solubility data we provide is not the same as the data used in the book,\n",
    "the best-performing models derived in the book may not yield the best results for you.\n",
    "However the models presented in the book should be very good starting points.\n",
    "\n",
    "<hr style=\"border-width:20px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  The Goal\n",
    "\n",
    "In this assignment you are to predict the solubility values for a set of test data:\n",
    "<ul><li>\n",
    "Given the file <tt>training_set</tt>, develop a regression model that is as accurate as possible.\n",
    "</li><li>\n",
    "Use your model to predict solubility for each row of data in <tt>test_set.csv</tt>.\n",
    "</li><li>\n",
    "Put your predictions in a .csv file called  <tt>HW5_Solubility_Predictions.csv</tt> and upload it to CCLE.\n",
    "</li></ul>\n",
    "\n",
    "<hr style=\"border-width:20px;\">\n",
    "\n",
    "## Step 1: get the datasets\n",
    "\n",
    "    \n",
    "The .zip file for this assignment has two csv data files:  a training set and a test set.\n",
    "\n",
    "<hr style=\"border-width:20px;\">\n",
    "\n",
    "## Step 2: construct a model from <tt>training_set.csv</tt>\n",
    "\n",
    "Using the <tt>training_set.csv</tt> data, construct a regression model.\n",
    "\n",
    "<br/>\n",
    "<b>YOU CAN USE ANY ENVIRONMENT YOU LIKE TO BUILD A REGRESSION MODEL.</b>\n",
    "Please construct the most accurate models you can.\n",
    "\n",
    "<hr style=\"border-width:20px;\">\n",
    "\n",
    "## Step 3: generate predictions from <tt>test_set.csv</tt>\n",
    "    \n",
    "The rows of file <tt>test_set.csv</tt> have input features for a number of molecules.\n",
    "Using your classifer, produce solubility predictions for each of them.\n",
    "\n",
    "<br/>\n",
    "Put one predicted class name per line in a CSV file <tt>HW5_Solubility_Predictions.csv</tt>.\n",
    "This file should also have the header line \"<tt>Solubility</tt>\".\n",
    "\n",
    "<br/>\n",
    "<i>Your score on this problem will be the R-squared value of these predictions.</i>\n",
    "<br/>\n",
    "\n",
    "<hr style=\"border-width:20px;\">\n",
    "\n",
    "## Step 4: upload <tt>HW5_Solubility_Predictions.csv</tt> and your notebook to CCLE\n",
    "\n",
    "Finally, go to CCLE and upload:\n",
    "<ul><li>\n",
    "your output CSV file <tt>HW5_Solubility_Predictions.csv</tt>\n",
    "</li><li>\n",
    "your notebook file <tt>HW5_Solubility_Predictions.ipynb</tt>\n",
    "</li></ul>\n",
    "\n",
    "We are not planning to run any of the uploaded notebooks.\n",
    "However, your notebook should have the commands you used in developing your models ---\n",
    "in order to show your work.\n",
    "As announced, all assignment grading in this course will be automated,\n",
    "and the notebook is needed in order to check results of the grading program.\n",
    "\n",
    "<hr style=\"border-width:20px;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Appied Predictive Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An integrated package for supervised learning, using over 50 kinds of\n",
    "models, and a variety of different metrics:</p>\n",
    "\n",
    "<pre><code>  Applied Predictive Modeling\n",
    "  M. Kuhn and K. Johnson\n",
    "  Springer-Verlag, 2013.\n",
    "  ISBN: 978-1-4614-6848-6 (Print)\n",
    "\n",
    "</code></pre>\n",
    "<p><a href=\"http://link.springer.com/book/10.1007%2F978-1-4614-6849-3\">http://link.springer.com/book/10.1007%2F978-1-4614-6849-3</a></p>\n",
    "<p>[APM] is similar to [ISL] and [ESL] but emphasizes practical model\n",
    "development/evaluation, including case histories with R scripts.</p>\n",
    "<p>Its <tt>caret</tt> library (<a href=\"http://caret.r-forge.r-project.org\">http://caret.r-forge.r-project.org</a>) is integrated with R packages that support Supervised Learning using\n",
    "mainstream model evaluation methods and  100 popular models.</p>\n",
    "<p>caret package manual (PDF):  <a href=\"http://cran.r-project.org/web/packages/caret/caret.pdf\">http://cran.r-project.org/web/packages/caret/caret.pdf</a></p>\n",
    "<p>List of models in caret (reproduced as a table below):\n",
    "<a href=\"http://caret.r-forge.r-project.org/modelList.html\">http://caret.r-forge.r-project.org/modelList.html</a></p>\n",
    "<p>caret package overview: <a href=\"http://www.jstatsoft.org/v28/i05/paper\">http://www.jstatsoft.org/v28/i05/paper</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "not.installed <- function(pkg) !is.element(pkg, installed.packages()[,1])\n",
    "    \n",
    "if (not.installed(\"caret\")) install.packages(\"caret\", repos=\"http://cran.us.r-project.org\")\n",
    "\n",
    "library(caret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# library(help=caret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  NOTE:  YOU WILL NEED TO INSTALL ALL OF THE PACKAGES USED IN THE BOOK !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if (not.installed(\"AppliedPredictiveModeling\")) {\n",
    "    \n",
    "    install.packages(\"AppliedPredictiveModeling\")\n",
    "    \n",
    "    \n",
    "    library(AppliedPredictiveModeling)\n",
    "    \n",
    "    \n",
    "    for (chapter in c(2,3,4,6,7,8,10, 11,12,13,14,16,17,19))  getPackages(chapter)  # this takes a while\n",
    "\n",
    "        \n",
    "        \n",
    "} else {\n",
    "\n",
    "    library(AppliedPredictiveModeling)\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# library(help=AppliedPredictiveModeling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Grid Search is often used in APM to search a model's parameter space, and\n",
    "# some chapters use the \"doMC\" package to do Multi-Core computation\n",
    "# (supported only on Linux or MacOS):\n",
    "\n",
    "if (not.installed(\"doMC\"))  install.packages(\"doMC\")   # multicore computation in R\n",
    "\n",
    "library(doMC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# library(help=doMC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chapters use the following models:</p>\n",
    "<pre>\n",
    "02_A_Short_Tour.R           lm, earth\n",
    "04_Over_Fitting.R           svmRadial, glm\n",
    "06_Linear_Regression.R      lm, pls, pcr, ridge, enet\n",
    "07_Non-Linear_Reg.R         avNNet, earth, svmRadial, svmPoly, knn\n",
    "08_Regression_Trees.R       rpart, ctree, M5, treebag, rf, cforest, gbm\n",
    "10_Case_Study_Concrete.R    lm, pls, enet, earth, svmRadial, avNNet, rpart,\n",
    "                            treebag, ctree, rf, gbm, cubist, M5, Nelder-Mead\n",
    "11_Class_Performance.R      glm\n",
    "12_Discriminant_Analysis.R  svmRadial, glm, lda, pls, glmnet, pam\n",
    "13_Non-Linear_Class.R       mda, nnet, avNNet, fda, svmRadial, svmPoly, knn, nb\n",
    "14_Class_Trees.R            rpart, J48, PART, treebag, rf, gbm, C5.0\n",
    "16_Class_Imbalance.R        rf, glm, fda, svmRadial, rpart, C5.0\n",
    "17_Job_Scheduling.R         rpart, lda, sparseLDA, nnet, pls, fda, rf, C5.0,\n",
    "                            treebag, svmRadial\n",
    "19_Feature_Select.R         rf, lda, svmRadial, nb, glm, knn, svmRadial, knn\n",
    "</pre>\n",
    "\n",
    "<p>Training control methods used by the scripts:</p>\n",
    "<pre>\n",
    "04_Over_Fitting.R           repeatedcv, cv, LOOCV, LGOCV, boot, boot632\n",
    "06_Linear_Regression.R      cv\n",
    "07_Non-Linear_Reg.R         cv\n",
    "08_Regression_Trees.R       cv, oob\n",
    "10_Case_Study_Concrete.R    repeatedcv\n",
    "11_Class_Performance.R      repeatedcv\n",
    "12_Discriminant_Analysis.R  cv, LGOCV\n",
    "13_Non-Linear_Class.R       LGOCV\n",
    "14_Class_Trees.R            LGOCV\n",
    "16_Class_Imbalance.R        cv\n",
    "17_Job_Scheduling.R         repeatedcv\n",
    "19_Feature_Select.R         repeatedcv, cv\n",
    "</pre>\n",
    "\n",
    "\n",
    "APMchapters = c(\n",
    "\"\",\n",
    "\"02_A_Short_Tour.R\",\n",
    "\"03_Data_Pre_Processing.R\",\n",
    "\"04_Over_Fitting.R\",\n",
    "\"\",\n",
    "\"06_Linear_Regression.R\",\n",
    "\"07_Non-Linear_Reg.R\",\n",
    "\"08_Regression_Trees.R\",\n",
    "\"\",\n",
    "\"10_Case_Study_Concrete.R\",\n",
    "\"11_Class_Performance.R\",\n",
    "\"12_Discriminant_Analysis.R\",\n",
    "\"13_Non-Linear_Class.R\",\n",
    "\"14_Class_Trees.R\",\n",
    "\"\",\n",
    "\"16_Class_Imbalance.R\",\n",
    "\"17_Job_Scheduling.R\",\n",
    "\"18_Importance.R\",\n",
    "\"19_Feature_Select.R\",\n",
    "\"CreateGrantData.R\")\n",
    "\n",
    "showChapterScript = function(n) {\n",
    "  if (APMchapters[n] != \"\")\n",
    "    file.show( file.path( scriptLocation(), APMchapters[n] ))\n",
    "}\n",
    "\n",
    "showChapterOutput = function(n) {\n",
    "  if (APMchapters[n] != \"\")\n",
    "    file.show( file.path( scriptLocation(), paste(APMchapters[n],\"out\",sep=\"\") ))\n",
    "}\n",
    "\n",
    "runChapterScript = function(n) {\n",
    "  if (APMchapters[n] != \"\")\n",
    "    source( file.path( scriptLocation(), APMchapters[n] ),  echo=TRUE )\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "showChapterScript(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#  runChapterScript(2)   #  don't run -- currently seems to die when executing  plot(marsFit)\n",
    "\n",
    "##     user  system elapsed \n",
    "##    4.971   0.114   5.292"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Another way to run the script for Chapter 2:\n",
    "\n",
    "PATIENT = FALSE\n",
    "\n",
    "if (PATIENT) {\n",
    "   current_working_directory = getwd()  # remember current directory\n",
    "\n",
    "   chapter_code_directory = scriptLocation()\n",
    "\n",
    "   setwd( chapter_code_directory )\n",
    "   print(dir())\n",
    "\n",
    "   print(source(\"02_A_Short_Tour.R\", echo=TRUE))\n",
    "\n",
    "   setwd(current_working_directory)  # return to working directory\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Another way to run the Chapter 2 script\n",
    "\n",
    "library(AppliedPredictiveModeling)\n",
    "data(FuelEconomy)\n",
    "\n",
    "## Format data for plotting against engine displacement\n",
    "\n",
    "## Sort by engine displacement\n",
    "cars2010 <- cars2010[order(cars2010$EngDispl),]\n",
    "cars2011 <- cars2011[order(cars2011$EngDispl),]\n",
    "\n",
    "## Combine data into one data frame\n",
    "cars2010a <- cars2010\n",
    "cars2010a$Year <- \"2010 Model Year\"\n",
    "cars2011a <- cars2011\n",
    "cars2011a$Year <- \"2011 Model Year\"\n",
    "\n",
    "plotData <- rbind(cars2010a, cars2011a)\n",
    "\n",
    "library(lattice)\n",
    "\n",
    "print(\n",
    "    xyplot(FE ~ EngDispl|Year, plotData,\n",
    "       xlab = \"Engine Displacement\",\n",
    "       ylab = \"Fuel Efficiency (MPG)\",\n",
    "       between = list(x = 1.2))\n",
    ")\n",
    "\n",
    "##########  'plot' routines in the lattice package must be print'ed to obtain their output !\n",
    "\n",
    "## Fit a single linear model and conduct 10-fold CV to estimate the error\n",
    "\n",
    "library(caret)\n",
    "set.seed(1)\n",
    "lm1Fit <- train(FE ~ EngDispl,\n",
    "                data = cars2010,\n",
    "                method = \"lm\",\n",
    "                trControl = trainControl(method= \"cv\"))\n",
    "print(lm1Fit)\n",
    "\n",
    "\n",
    "## Fit a quadratic model too\n",
    "\n",
    "## Create squared terms\n",
    "cars2010$ED2 <- cars2010$EngDispl^2\n",
    "cars2011$ED2 <- cars2011$EngDispl^2\n",
    "\n",
    "set.seed(1)\n",
    "lm2Fit <- train(FE ~ EngDispl + ED2,\n",
    "                data = cars2010,\n",
    "                method = \"lm\",\n",
    "                trControl = trainControl(method= \"cv\"))\n",
    "print(lm2Fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Finally a MARS model (via the earth package)\n",
    "\n",
    "library(earth)\n",
    "set.seed(1)\n",
    "marsFit <- train(FE ~ EngDispl,\n",
    "                 data = cars2010,\n",
    "                 method = \"earth\",\n",
    "                 tuneLength = 15,\n",
    "                 trControl = trainControl(method= \"cv\"))\n",
    "print(marsFit)\n",
    "\n",
    "\n",
    "plot(marsFit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Predict the test set data\n",
    "cars2011$lm1  <- predict(lm1Fit,  cars2011)\n",
    "cars2011$lm2  <- predict(lm2Fit,  cars2011)\n",
    "cars2011$mars <- predict(marsFit, cars2011)\n",
    "\n",
    "## Get test set performance values via caret's postResample function\n",
    "\n",
    "print(postResample(pred = cars2011$lm1,  obs = cars2011$FE))\n",
    "print(postResample(pred = cars2011$lm2,  obs = cars2011$FE))\n",
    "print(postResample(pred = cars2011$mars, obs = cars2011$FE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "showChapterScript(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "showChapterOutput(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "showChapterScript(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "showChapterOutput(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "runChapterScript(3)\n",
    "\n",
    "##    user  system elapsed \n",
    "##   5.791   0.147   6.146 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Section 3.1 Case Study: Cell Segmentation in High-Content Screening\n",
    "\n",
    "library(AppliedPredictiveModeling)\n",
    "data(segmentationOriginal)\n",
    "\n",
    "## Retain the original training set\n",
    "segTrain <- subset(segmentationOriginal, Case == \"Train\")\n",
    "\n",
    "## Remove the first three columns (identifier columns)\n",
    "segTrainX <- segTrain[, -(1:3)]\n",
    "segTrainClass <- segTrain$Class\n",
    "\n",
    "colnames(segTrain)\n",
    "\n",
    "table(segTrainClass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Section 3.2 Data Transformations for Individual Predictors\n",
    "\n",
    "## The column VarIntenCh3 measures the standard deviation of the intensity\n",
    "## of the pixels in the actin filaments\n",
    "\n",
    "max(segTrainX$VarIntenCh3)/min(segTrainX$VarIntenCh3)\n",
    "\n",
    "library(e1071)\n",
    "skewness(segTrainX$VarIntenCh3)\n",
    "\n",
    "library(caret)\n",
    "\n",
    "## Use caret's preProcess function to transform for skewness\n",
    "segPP <- preProcess(segTrainX, method = \"BoxCox\")\n",
    "\n",
    "## Apply the transformations\n",
    "segTrainTrans <- predict(segPP, segTrainX)\n",
    "\n",
    "## Results for a single predictor\n",
    "segPP$bc$VarIntenCh3\n",
    "\n",
    "histogram(~segTrainX$VarIntenCh3,\n",
    "          xlab = \"Natural Units\",\n",
    "          type = \"count\")\n",
    "\n",
    "histogram(~log(segTrainX$VarIntenCh3),\n",
    "          xlab = \"Log Units\",\n",
    "          ylab = \" \",\n",
    "          type = \"count\")\n",
    "segPP$bc$PerimCh1\n",
    "\n",
    "histogram(~segTrainX$PerimCh1,\n",
    "          xlab = \"Natural Units\",\n",
    "          type = \"count\")\n",
    "\n",
    "histogram(~segTrainTrans$PerimCh1,\n",
    "          xlab = \"Transformed Data\",\n",
    "          ylab = \" \",\n",
    "          type = \"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Section 3.3 Data Transformations for Multiple Predictors\n",
    "\n",
    "## R's prcomp is used to conduct PCA\n",
    "pr <- prcomp(~ AvgIntenCh1 + EntropyIntenCh1,\n",
    "             data = segTrainTrans,\n",
    "             scale. = TRUE)\n",
    "\n",
    "transparentTheme(pchSize = .7, trans = .3)\n",
    "\n",
    "xyplot(AvgIntenCh1 ~ EntropyIntenCh1,\n",
    "       data = segTrainTrans,\n",
    "       groups = segTrain$Class,\n",
    "       xlab = \"Channel 1 Fiber Width\",\n",
    "       ylab = \"Intensity Entropy Channel 1\",\n",
    "       auto.key = list(columns = 2),\n",
    "       type = c(\"p\", \"g\"),\n",
    "       main = \"Original Data\",\n",
    "       aspect = 1)\n",
    "\n",
    "xyplot(PC2 ~ PC1,\n",
    "       data = as.data.frame(pr$x),\n",
    "       groups = segTrain$Class,\n",
    "       xlab = \"Principal Component #1\",\n",
    "       ylab = \"Principal Component #2\",\n",
    "       main = \"Transformed\",\n",
    "       xlim = extendrange(pr$x),\n",
    "       ylim = extendrange(pr$x),\n",
    "       type = c(\"p\", \"g\"),\n",
    "       aspect = 1)\n",
    "\n",
    "## Apply PCA to the entire set of predictors.\n",
    "\n",
    "## There are a few predictors with only a single value, so we remove these first\n",
    "## (since PCA uses variances, which would be zero)\n",
    "\n",
    "isZV <- apply(segTrainX, 2, function(x) length(unique(x)) == 1)\n",
    "segTrainX <- segTrainX[, !isZV]\n",
    "\n",
    "segPP <- preProcess(segTrainX, c(\"BoxCox\", \"center\", \"scale\"))\n",
    "segTrainTrans <- predict(segPP, segTrainX)\n",
    "\n",
    "segPCA <- prcomp(segTrainTrans, center = TRUE, scale. = TRUE)\n",
    "\n",
    "## Plot a scatterplot matrix of the first three components\n",
    "transparentTheme(pchSize = .8, trans = .3)\n",
    "\n",
    "panelRange <- extendrange(segPCA$x[, 1:3])\n",
    "\n",
    "splom(as.data.frame(segPCA$x[, 1:3]),\n",
    "      groups = segTrainClass,\n",
    "      type = c(\"p\", \"g\"),\n",
    "      as.table = TRUE,\n",
    "      auto.key = list(columns = 2),\n",
    "      prepanel.limits = function(x) panelRange)\n",
    "\n",
    "## Format the rotation values for plotting\n",
    "segRot <- as.data.frame(segPCA$rotation[, 1:3])\n",
    "\n",
    "## Derive the channel variable\n",
    "vars <- rownames(segPCA$rotation)\n",
    "channel <- rep(NA, length(vars))\n",
    "channel[grepl(\"Ch1$\", vars)] <- \"Channel 1\"\n",
    "channel[grepl(\"Ch2$\", vars)] <- \"Channel 2\"\n",
    "channel[grepl(\"Ch3$\", vars)] <- \"Channel 3\"\n",
    "channel[grepl(\"Ch4$\", vars)] <- \"Channel 4\"\n",
    "\n",
    "segRot$Channel <- channel\n",
    "segRot <- segRot[complete.cases(segRot),]\n",
    "\n",
    "segRot$Channel <- factor(as.character(segRot$Channel))\n",
    "\n",
    "## Plot a scatterplot matrix of the first three rotation variables\n",
    "\n",
    "transparentTheme(pchSize = .8, trans = .7)\n",
    "panelRange <- extendrange(segRot[, 1:3])\n",
    "library(ellipse)\n",
    "upperp <- function(...)\n",
    "  {\n",
    "    args <- list(...)\n",
    "    circ1 <- ellipse(diag(rep(1, 2)), t = .1)\n",
    "    panel.xyplot(circ1[,1], circ1[,2],\n",
    "                 type = \"l\",\n",
    "                 lty = trellis.par.get(\"reference.line\")$lty,\n",
    "                 col = trellis.par.get(\"reference.line\")$col,\n",
    "                 lwd = trellis.par.get(\"reference.line\")$lwd)\n",
    "    circ2 <- ellipse(diag(rep(1, 2)), t = .2)\n",
    "    panel.xyplot(circ2[,1], circ2[,2],\n",
    "                 type = \"l\",\n",
    "                 lty = trellis.par.get(\"reference.line\")$lty,\n",
    "                 col = trellis.par.get(\"reference.line\")$col,\n",
    "                 lwd = trellis.par.get(\"reference.line\")$lwd)\n",
    "    circ3 <- ellipse(diag(rep(1, 2)), t = .3)\n",
    "    panel.xyplot(circ3[,1], circ3[,2],\n",
    "                 type = \"l\",\n",
    "                 lty = trellis.par.get(\"reference.line\")$lty,\n",
    "                 col = trellis.par.get(\"reference.line\")$col,\n",
    "                 lwd = trellis.par.get(\"reference.line\")$lwd)\n",
    "    panel.xyplot(args$x, args$y, groups = args$groups, subscripts = args$subscripts)\n",
    "  }\n",
    "          \n",
    "splom(~segRot[, 1:3],\n",
    "      groups = segRot$Channel,\n",
    "      lower.panel = function(...){}, upper.panel = upperp,\n",
    "      prepanel.limits = function(x) panelRange,\n",
    "      auto.key = list(columns = 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Section 3.5 Removing Variables\n",
    "\n",
    "## To filter on correlations, we first get the correlation matrix for the\n",
    "## predictor set\n",
    "\n",
    "segCorr <- cor(segTrainTrans)\n",
    "\n",
    "library(corrplot)\n",
    "corrplot(segCorr, order = \"hclust\", tl.cex = .35)\n",
    "\n",
    "## caret's findCorrelation function is used to identify columns to remove.\n",
    "highCorr <- findCorrelation(segCorr, .75)\n",
    "\n",
    "highCorr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Section 3.8 Computing (Creating Dummy Variables)\n",
    "\n",
    "data(cars)\n",
    "type <- c(\"convertible\", \"coupe\", \"hatchback\", \"sedan\", \"wagon\")\n",
    "cars$Type <- factor(apply(cars[, 14:18], 1, function(x) type[which(x == 1)]))\n",
    "\n",
    "carSubset <- cars[sample(1:nrow(cars), 20), c(1, 2, 19)]\n",
    "\n",
    "head(carSubset)\n",
    "levels(carSubset$Type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "simpleMod <- dummyVars(~Mileage + Type,\n",
    "                       data = carSubset,\n",
    "                       ## Remove the variable name from the\n",
    "                       ## column name\n",
    "                       levelsOnly = TRUE)\n",
    "    simpleMod\n",
    "\n",
    "withInteraction <- dummyVars(~Mileage + Type + Mileage:Type,\n",
    "                             data = carSubset,\n",
    "                             levelsOnly = TRUE)\n",
    "withInteraction\n",
    "predict(withInteraction, head(carSubset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "showChapterScript(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "showChapterOutput(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PATIENT = FALSE\n",
    "\n",
    "if (PATIENT) {\n",
    "    runChapterScript(4)\n",
    "}\n",
    "\n",
    "#  this required about an hour of computation\n",
    "\n",
    "minutes_required_for_this_script = 3260.432 / 60 \n",
    "minutes_required_for_this_script\n",
    "\n",
    "## user   system  elapsed \n",
    "## 3260.432  211.968  906.933 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######## This computation can take five minutes to complete on a single cpu.\n",
    "\n",
    "### Section 4.6 Choosing Final Tuning Parameters\n",
    "\n",
    "detach(package:caret)  # reload the package, since the code here modifies GermanCredit\n",
    "library(caret)\n",
    "data(GermanCredit)\n",
    "\n",
    "## First, remove near-zero variance predictors then get rid of a few predictors\n",
    "## that duplicate values. For example, there are two possible values for the\n",
    "## housing variable: \"Rent\", \"Own\" and \"ForFree\". So that we don't have linear\n",
    "## dependencies, we get rid of one of the levels (e.g. \"ForFree\")\n",
    "\n",
    "GermanCredit <- GermanCredit[, -nearZeroVar(GermanCredit)]\n",
    "GermanCredit$CheckingAccountStatus.lt.0 <- NULL\n",
    "GermanCredit$SavingsAccountBonds.lt.100 <- NULL\n",
    "GermanCredit$EmploymentDuration.lt.1 <- NULL\n",
    "GermanCredit$EmploymentDuration.Unemployed <- NULL\n",
    "GermanCredit$Personal.Male.Married.Widowed <- NULL\n",
    "GermanCredit$Property.Unknown <- NULL\n",
    "GermanCredit$Housing.ForFree <- NULL\n",
    "\n",
    "## Split the data into training (80%) and test sets (20%)\n",
    "set.seed(100)\n",
    "inTrain <- createDataPartition(GermanCredit$Class, p = .8)[[1]]\n",
    "GermanCreditTrain <- GermanCredit[ inTrain, ]\n",
    "GermanCreditTest  <- GermanCredit[-inTrain, ]\n",
    "\n",
    "## The model fitting code shown in the computing section is fairly\n",
    "## simplistic.  For the text we estimate the tuning parameter grid\n",
    "## up-front and pass it in explicitly. This generally is not needed,\n",
    "## but was used here so that we could trim the cost values to a\n",
    "## presentable range and to re-use later with different resampling\n",
    "## methods.\n",
    "\n",
    "library(kernlab)\n",
    "set.seed(231)\n",
    "sigDist <- sigest(Class ~ ., data = GermanCreditTrain, frac = 1)\n",
    "svmTuneGrid <- data.frame(sigma = as.vector(sigDist)[1], C = 2^(-2:7))\n",
    "\n",
    "### Optional: parallel processing can be used via the 'do' packages,\n",
    "### such as doMC, doMPI etc. We used doMC (not on Windows) to speed\n",
    "### up the computations.\n",
    "\n",
    "### WARNING: Be aware of how much memory is needed to parallel\n",
    "### process. It can very quickly overwhelm the available hardware. We\n",
    "### estimate the memory usage (VSIZE = total memory size) to be\n",
    "### 2566M/core.\n",
    "\n",
    "### library(doMC)\n",
    "### registerDoMC(4)\n",
    "\n",
    "set.seed(1056)\n",
    "svmFit <- train(Class ~ .,\n",
    "                data = GermanCreditTrain,\n",
    "                method = \"svmRadial\",\n",
    "                preProc = c(\"center\", \"scale\"),\n",
    "                tuneGrid = svmTuneGrid,\n",
    "                trControl = trainControl(method = \"repeatedcv\",\n",
    "                                         repeats = 5,\n",
    "                                         classProbs = TRUE))\n",
    "## classProbs = TRUE was added since the text was written\n",
    "\n",
    "## Print the results\n",
    "svmFit\n",
    "\n",
    "## A line plot of the average performance. The 'scales' argument is actually an\n",
    "## argument to xyplot that converts the x-axis to log-2 units.\n",
    "\n",
    "plot(svmFit, scales = list(x = list(log = 2)))\n",
    "\n",
    "## Test set predictions\n",
    "\n",
    "predictedClasses <- predict(svmFit, GermanCreditTest)\n",
    "str(predictedClasses)\n",
    "\n",
    "## Use the \"type\" option to get class probabilities\n",
    "\n",
    "predictedProbs <- predict(svmFit, newdata = GermanCreditTest, type = \"prob\")\n",
    "head(predictedProbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######## This computation can take over a half hour to complete on a single cpu.\n",
    "\n",
    "## Fit the same model using different resampling methods. The main syntax change\n",
    "## is the control object.\n",
    "\n",
    "set.seed(1056)\n",
    "svmFit10CV <- train(Class ~ .,\n",
    "                    data = GermanCreditTrain,\n",
    "                    method = \"svmRadial\",\n",
    "                    preProc = c(\"center\", \"scale\"),\n",
    "                    tuneGrid = svmTuneGrid,\n",
    "                    trControl = trainControl(method = \"cv\", number = 10))\n",
    "svmFit10CV\n",
    "\n",
    "set.seed(1056)\n",
    "svmFitLOO <- train(Class ~ .,\n",
    "                   data = GermanCreditTrain,\n",
    "                   method = \"svmRadial\",\n",
    "                   preProc = c(\"center\", \"scale\"),\n",
    "                   tuneGrid = svmTuneGrid,\n",
    "                   trControl = trainControl(method = \"LOOCV\"))\n",
    "svmFitLOO\n",
    "\n",
    "set.seed(1056)\n",
    "svmFitLGO <- train(Class ~ .,\n",
    "                   data = GermanCreditTrain,\n",
    "                   method = \"svmRadial\",\n",
    "                   preProc = c(\"center\", \"scale\"),\n",
    "                   tuneGrid = svmTuneGrid,\n",
    "                   trControl = trainControl(method = \"LGOCV\",\n",
    "                                            number = 50,\n",
    "                                            p = .8))\n",
    "svmFitLGO\n",
    "\n",
    "set.seed(1056)\n",
    "svmFitBoot <- train(Class ~ .,\n",
    "                    data = GermanCreditTrain,\n",
    "                    method = \"svmRadial\",\n",
    "                    preProc = c(\"center\", \"scale\"),\n",
    "                    tuneGrid = svmTuneGrid,\n",
    "                    trControl = trainControl(method = \"boot\", number = 50))\n",
    "svmFitBoot\n",
    "\n",
    "set.seed(1056)\n",
    "svmFitBoot632 <- train(Class ~ .,\n",
    "                       data = GermanCreditTrain,\n",
    "                       method = \"svmRadial\",\n",
    "                       preProc = c(\"center\", \"scale\"),\n",
    "                       tuneGrid = svmTuneGrid,\n",
    "                       trControl = trainControl(method = \"boot632\",\n",
    "                                                number = 50))\n",
    "svmFitBoot632"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Section 4.8 Choosing Between Models\n",
    "\n",
    "set.seed(1056)\n",
    "glmProfile <- train(Class ~ .,\n",
    "                    data = GermanCreditTrain,\n",
    "                    method = \"glm\",\n",
    "                    trControl = trainControl(method = \"repeatedcv\",\n",
    "                                             repeats = 5))\n",
    "glmProfile\n",
    "\n",
    "resamp <- resamples(list(SVM = svmFit, Logistic = glmProfile))\n",
    "summary(resamp)\n",
    "\n",
    "## These results are slightly different from those shown in the text.\n",
    "## There are some differences in the train() function since the\n",
    "## original results were produced. This is due to a difference in\n",
    "## predictions from the ksvm() function when class probs are requested\n",
    "## and when they are not. See, for example,\n",
    "## https://stat.ethz.ch/pipermail/r-help/2013-November/363188.html\n",
    "\n",
    "modelDifferences <- diff(resamp)\n",
    "summary(modelDifferences)\n",
    "\n",
    "## The actual paired t-test:\n",
    "modelDifferences$statistics$Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "showChapterScript(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "showChapterOutput(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "runChapterScript(6)\n",
    "\n",
    "##     user  system elapsed \n",
    "##  540.993  74.917 615.942 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Section 6.1 Case Study: Quantitative Structure- Activity\n",
    "### Relationship Modeling\n",
    "\n",
    "library(AppliedPredictiveModeling)\n",
    "data(solubility)\n",
    "\n",
    "library(lattice)\n",
    "\n",
    "### Some initial plots of the data\n",
    "xyplot(solTrainY ~ solTrainX$MolWeight, type = c(\"p\", \"g\"),\n",
    "       ylab = \"Solubility (log)\",\n",
    "       main = \"(a)\",\n",
    "       xlab = \"Molecular Weight\")\n",
    "\n",
    "xyplot(solTrainY ~ solTrainX$NumRotBonds, type = c(\"p\", \"g\"),\n",
    "       ylab = \"Solubility (log)\",\n",
    "       xlab = \"Number of Rotatable Bonds\")\n",
    "\n",
    "bwplot(solTrainY ~ ifelse(solTrainX[,100] == 1,\n",
    "                          \"structure present\",\n",
    "                          \"structure absent\"),\n",
    "       ylab = \"Solubility (log)\",\n",
    "       main = \"(b)\",\n",
    "       horizontal = FALSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Find the columns that are not fingerprints (i.e. the continuous\n",
    "### predictors). grep will return a list of integers corresponding to\n",
    "### column names that contain the pattern \"FP\".\n",
    "\n",
    "notFingerprints <- grep(\"FP\", names(solTrainXtrans))\n",
    "\n",
    "library(caret)\n",
    "\n",
    "featurePlot(solTrainXtrans[, -notFingerprints],\n",
    "            solTrainY,\n",
    "            between = list(x = 1, y = 1),\n",
    "            type = c(\"g\", \"p\", \"smooth\"),\n",
    "            labels = rep(\"\", 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "library(corrplot)\n",
    "\n",
    "### We used the full namespace to call this function because the pls\n",
    "### package (also used in this chapter) has a function with the same\n",
    "### name.\n",
    "\n",
    "corrplot::corrplot(cor(solTrainXtrans[, -notFingerprints]),\n",
    "                   order = \"hclust\",\n",
    "                   tl.cex = .8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Section 6.2 Linear Regression\n",
    "\n",
    "### Create a control function that will be used across models. We\n",
    "### create the fold assignments explicitly instead of relying on the\n",
    "### random number seed being set to identical values.\n",
    "\n",
    "set.seed(100)\n",
    "indx <- createFolds(solTrainY, returnTrain = TRUE)\n",
    "ctrl <- trainControl(method = \"cv\", index = indx)\n",
    "\n",
    "### Linear regression model with all of the predictors. This will\n",
    "### produce some warnings that a 'rank-deficient fit may be\n",
    "### misleading'. This is related to the predictors being so highly\n",
    "### correlated that some of the math has broken down.\n",
    "\n",
    "\n",
    "set.seed(100)\n",
    "lmTune0 <- train(x = solTrainXtrans, y = solTrainY,\n",
    "                 method = \"lm\",\n",
    "                 trControl = ctrl)\n",
    "\n",
    "lmTune0\n",
    "\n",
    "### And another using a set of predictors reduced by unsupervised\n",
    "### filtering. We apply a filter to reduce extreme between-predictor\n",
    "### correlations. Note the lack of warnings.\n",
    "\n",
    "tooHigh <- findCorrelation(cor(solTrainXtrans), .9)\n",
    "trainXfiltered <- solTrainXtrans[, -tooHigh]\n",
    "testXfiltered  <-  solTestXtrans[, -tooHigh]\n",
    "\n",
    "set.seed(100)\n",
    "lmTune <- train(x = trainXfiltered, y = solTrainY,\n",
    "                method = \"lm\",\n",
    "                trControl = ctrl)\n",
    "\n",
    "lmTune\n",
    "\n",
    "### Save the test set results in a data frame\n",
    "testResults <- data.frame(obs = solTestY,\n",
    "                          Linear_Regression = predict(lmTune, testXfiltered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Section 6.3 Partial Least Squares\n",
    "\n",
    "## Run PLS and PCR on solubility data and compare results\n",
    "set.seed(100)\n",
    "plsTune <- train(x = solTrainXtrans, y = solTrainY,\n",
    "                 method = \"pls\",\n",
    "                 tuneGrid = expand.grid(ncomp = 1:20),\n",
    "                 trControl = ctrl)\n",
    "\n",
    "plsTune\n",
    "\n",
    "\n",
    "testResults$PLS <- predict(plsTune, solTestXtrans)\n",
    "\n",
    "set.seed(100)\n",
    "pcrTune <- train(x = solTrainXtrans, y = solTrainY,\n",
    "                 method = \"pcr\",\n",
    "                 tuneGrid = expand.grid(ncomp = 1:35),\n",
    "                 trControl = ctrl)\n",
    "\n",
    "pcrTune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plsResamples <- plsTune$results\n",
    "plsResamples$Model <- \"PLS\"\n",
    "pcrResamples <- pcrTune$results\n",
    "pcrResamples$Model <- \"PCR\"\n",
    "plsPlotData <- rbind(plsResamples, pcrResamples)\n",
    "\n",
    "\n",
    "xyplot(RMSE ~ ncomp,\n",
    "       data = plsPlotData,\n",
    "       #aspect = 1,\n",
    "       xlab = \"# Components\",\n",
    "       ylab = \"RMSE (Cross-Validation)\",\n",
    "       auto.key = list(columns = 2),\n",
    "       groups = Model,\n",
    "       type = c(\"o\", \"g\"))\n",
    "\n",
    "\n",
    "plsImp <- varImp(plsTune, scale = FALSE)\n",
    "plot(plsImp, top = 25, scales = list(y = list(cex = .95)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Section 6.4 Penalized Models\n",
    "\n",
    "## The text used the elasticnet to obtain a ridge regression model.\n",
    "## There is now a simple ridge regression method.\n",
    "\n",
    "ridgeGrid <- expand.grid(lambda = seq(0, .1, length = 15))\n",
    "\n",
    "set.seed(100)\n",
    "ridgeTune <- train(x = solTrainXtrans, y = solTrainY,\n",
    "                   method = \"ridge\",\n",
    "                   tuneGrid = ridgeGrid,\n",
    "                   trControl = ctrl,\n",
    "                   preProc = c(\"center\", \"scale\"))\n",
    "\n",
    "ridgeTune\n",
    "\n",
    "\n",
    "update(plot(ridgeTune), xlab = \"Penalty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "enetGrid <- expand.grid(lambda = c(0, 0.01, .1),\n",
    "                        fraction = seq(.05, 1, length = 20))\n",
    "set.seed(100)\n",
    "enetTune <- train(x = solTrainXtrans, y = solTrainY,\n",
    "                  method = \"enet\",\n",
    "                  tuneGrid = enetGrid,\n",
    "                  trControl = ctrl,\n",
    "                  preProc = c(\"center\", \"scale\"))\n",
    "\n",
    "enetTune\n",
    "\n",
    "\n",
    "plot(enetTune)\n",
    "\n",
    "testResults$Enet <- predict(enetTune, solTestXtrans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "showChapterScript(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "showChapterOutput(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## runChapterScript(7)\n",
    "\n",
    "##        user     system    elapsed \n",
    "##  112106.723    188.979  12272.168 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "showChapterScript(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "showChapterOutput(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##  runChapterScript(8)\n",
    "\n",
    "##       user    system   elapsed \n",
    "##  21280.849   500.609  6798.887 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "library(AppliedPredictiveModeling)\n",
    "data(solubility)\n",
    "\n",
    "### Create a control function that will be used across models. We\n",
    "### create the fold assignments explicitly instead of relying on the\n",
    "### random number seed being set to identical values.\n",
    "\n",
    "library(caret)\n",
    "set.seed(100)\n",
    "indx <- createFolds(solTrainY, returnTrain = TRUE)\n",
    "ctrl <- trainControl(method = \"cv\", index = indx)\n",
    "\n",
    "################################################################################\n",
    "### Section 8.1 Basic Regression Trees\n",
    "\n",
    "library(rpart)\n",
    "\n",
    "### Fit two CART models to show the initial splitting process. rpart\n",
    "### only uses formulas, so we put the predictors and outcome into\n",
    "### a common data frame first.\n",
    "\n",
    "trainData <- solTrainXtrans\n",
    "trainData$y <- solTrainY\n",
    "\n",
    "rpStump <- rpart(y ~ ., data = trainData,\n",
    "                 control = rpart.control(maxdepth = 1))\n",
    "rpSmall <- rpart(y ~ ., data = trainData,\n",
    "                 control = rpart.control(maxdepth = 2))\n",
    "\n",
    "### Tune the model\n",
    "library(caret)\n",
    "\n",
    "set.seed(100)\n",
    "cartTune <- train(x = solTrainXtrans, y = solTrainY,\n",
    "                  method = \"rpart\",\n",
    "                  tuneLength = 25,\n",
    "                  trControl = ctrl)\n",
    "cartTune\n",
    "## cartTune$finalModel\n",
    "\n",
    "### Plot the tuning results\n",
    "plot(cartTune, scales = list(x = list(log = 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Use the partykit package to make some nice plots. First, convert\n",
    "### the rpart objects to party objects.\n",
    "\n",
    "# library(partykit)\n",
    "#\n",
    "# cartTree <- as.party(cartTune$finalModel)\n",
    "# plot(cartTree)\n",
    "\n",
    "### Get the variable importance. 'competes' is an argument that\n",
    "### controls whether splits not used in the tree should be included\n",
    "### in the importance calculations.\n",
    "\n",
    "cartImp <- varImp(cartTune, scale = FALSE, competes = FALSE)\n",
    "cartImp\n",
    "\n",
    "### Save the test set results in a data frame\n",
    "testResults <- data.frame(obs = solTestY,\n",
    "                          CART = predict(cartTune, solTestXtrans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot(cartImp, cex=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Tune the conditional inference tree\n",
    "\n",
    "cGrid <- data.frame(mincriterion = sort(c(.95, seq(.75, .99, length = 2))))\n",
    "\n",
    "set.seed(100)\n",
    "ctreeTune <- train(x = solTrainXtrans, y = solTrainY,\n",
    "                   method = \"ctree\",\n",
    "                   tuneGrid = cGrid,\n",
    "                   trControl = ctrl)\n",
    "ctreeTune\n",
    "plot(ctreeTune)\n",
    "\n",
    "##ctreeTune$finalModel\n",
    "plot(ctreeTune$finalModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testResults$cTree <- predict(ctreeTune, solTestXtrans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "showChapterScript(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "showChapterOutput(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Try this if you are very patient --\n",
    "# in the APM version of the output file:\n",
    "\n",
    "##############   THE RUN TIME FOR THIS SCRIPT IS LISTED AS 5.6 HOURS.\n",
    "\n",
    "# Chs 10 and 17 evaluate many different models in case studies.\n",
    "# To run the Ch.10 script:\n",
    "\n",
    "VERY_PATIENT = FALSE\n",
    "\n",
    "if (VERY_PATIENT) {\n",
    "   current_working_directory = getwd()  # remember current directory\n",
    "\n",
    "   chapter_code_directory = scriptLocation()\n",
    "\n",
    "   setwd( chapter_code_directory )\n",
    "   dir()\n",
    "\n",
    "   source(\"10_Case_Study_Concrete.R\", echo=TRUE)\n",
    "\n",
    "   setwd(current_working_directory)  # return to working directory\n",
    "}\n",
    "\n",
    "##       user    system   elapsed \n",
    "##  20277.196   121.470  4043.395 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "showChapterScript(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "showChapterOutput(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "runChapterScript(11)\n",
    "\n",
    "##     user  system elapsed \n",
    "##   11.120   0.526  11.698 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Section 11.1 Class Predictions\n",
    "\n",
    "library(AppliedPredictiveModeling)\n",
    "\n",
    "### Simulate some two class data with two predictors\n",
    "set.seed(975)\n",
    "training <- quadBoundaryFunc(500)\n",
    "testing <- quadBoundaryFunc(1000)\n",
    "testing$class2 <- ifelse(testing$class == \"Class1\", 1, 0)\n",
    "testing$ID <- 1:nrow(testing)\n",
    "\n",
    "### Fit models\n",
    "library(MASS)\n",
    "qdaFit <- qda(class ~ X1 + X2, data = training)\n",
    "\n",
    "library(randomForest)\n",
    "rfFit <- randomForest(class ~ X1 + X2, data = training, ntree = 2000)\n",
    "\n",
    "### Predict the test set\n",
    "testing$qda <- predict(qdaFit, testing)$posterior[,1]\n",
    "testing$rf <- predict(rfFit, testing, type = \"prob\")[,1]\n",
    "\n",
    "\n",
    "### Generate the calibration analysis\n",
    "library(caret)\n",
    "calData1 <- calibration(class ~ qda + rf, data = testing, cuts = 10)\n",
    "\n",
    "### Plot the curve\n",
    "\n",
    "xyplot(calData1, auto.key = list(columns = 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### To calibrate the data, treat the probabilities as inputs into the\n",
    "### model\n",
    "\n",
    "trainProbs <- training\n",
    "trainProbs$qda <- predict(qdaFit)$posterior[,1]\n",
    "\n",
    "### These models take the probabilities as inputs and, based on the\n",
    "### true class, re-calibrate them.\n",
    "library(klaR)\n",
    "nbCal <- NaiveBayes(class ~ qda, data = trainProbs, usekernel = TRUE)\n",
    "\n",
    "### We use relevel() here because glm() models the probability of the\n",
    "### second factor level.\n",
    "lrCal <- glm(relevel(class, \"Class2\") ~ qda, data = trainProbs, family = binomial)\n",
    "\n",
    "### Now re-predict the test set using the modified class probability\n",
    "### estimates\n",
    "testing$qda2 <- predict(nbCal, testing[, \"qda\", drop = FALSE])$posterior[,1]\n",
    "testing$qda3 <- predict(lrCal, testing[, \"qda\", drop = FALSE], type = \"response\")\n",
    "\n",
    "\n",
    "### Manipulate the data a bit for pretty plotting\n",
    "simulatedProbs <- testing[, c(\"class\", \"rf\", \"qda3\")]\n",
    "names(simulatedProbs) <- c(\"TrueClass\", \"RandomForestProb\", \"QDACalibrated\")\n",
    "simulatedProbs$RandomForestClass <-  predict(rfFit, testing)\n",
    "\n",
    "calData2 <- calibration(class ~ qda + qda2 + qda3, data = testing)\n",
    "calData2$data$calibModelVar <- as.character(calData2$data$calibModelVar)\n",
    "calData2$data$calibModelVar <- ifelse(calData2$data$calibModelVar == \"qda\",\n",
    "                                      \"QDA\",\n",
    "                                      calData2$data$calibModelVar)\n",
    "calData2$data$calibModelVar <- ifelse(calData2$data$calibModelVar == \"qda2\",\n",
    "                                      \"Bayesian Calibration\",\n",
    "                                      calData2$data$calibModelVar)\n",
    "\n",
    "calData2$data$calibModelVar <- ifelse(calData2$data$calibModelVar == \"qda3\",\n",
    "                                      \"Sigmoidal Calibration\",\n",
    "                                      calData2$data$calibModelVar)\n",
    "\n",
    "calData2$data$calibModelVar <- factor(calData2$data$calibModelVar,\n",
    "                                      levels = c(\"QDA\",\n",
    "                                                 \"Bayesian Calibration\",\n",
    "                                                 \"Sigmoidal Calibration\"))\n",
    "\n",
    "xyplot(calData2, auto.key = list(columns = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## These commands are needed to reload GermanCredit, which is changed by this and Ch.4 code:\n",
    "\n",
    "detach(package:caret)\n",
    "library(caret)\n",
    "data(GermanCredit)\n",
    "\n",
    "## First, remove near-zero variance predictors then get rid of a few predictors\n",
    "## that duplicate values. For example, there are two possible values for the\n",
    "## housing variable: \"Rent\", \"Own\" and \"ForFree\". So that we don't have linear\n",
    "## dependencies, we get rid of one of the levels (e.g. \"ForFree\")\n",
    "\n",
    "GermanCredit <- GermanCredit[, -nearZeroVar(GermanCredit)]\n",
    "GermanCredit$CheckingAccountStatus.lt.0 <- NULL\n",
    "GermanCredit$SavingsAccountBonds.lt.100 <- NULL\n",
    "GermanCredit$EmploymentDuration.lt.1 <- NULL\n",
    "GermanCredit$EmploymentDuration.Unemployed <- NULL\n",
    "GermanCredit$Personal.Male.Married.Widowed <- NULL\n",
    "GermanCredit$Property.Unknown <- NULL\n",
    "GermanCredit$Housing.ForFree <- NULL\n",
    "\n",
    "## Split the data into training (80%) and test sets (20%)\n",
    "set.seed(100)\n",
    "inTrain <- createDataPartition(GermanCredit$Class, p = .8)[[1]]\n",
    "GermanCreditTrain <- GermanCredit[ inTrain, ]\n",
    "GermanCreditTest  <- GermanCredit[-inTrain, ]\n",
    "\n",
    "set.seed(1056)\n",
    "logisticReg <- train(Class ~ .,\n",
    "                     data = GermanCreditTrain,\n",
    "                     method = \"glm\",\n",
    "                     trControl = trainControl(method = \"repeatedcv\",\n",
    "                                              repeats = 5))\n",
    "\n",
    "logisticReg\n",
    "\n",
    "\n",
    "### Predict the test set\n",
    "creditResults <- data.frame(obs = GermanCreditTest$Class)\n",
    "creditResults$prob <- predict(logisticReg, GermanCreditTest, type = \"prob\")[, \"Bad\"]\n",
    "creditResults$pred <- predict(logisticReg, GermanCreditTest)\n",
    "creditResults$Label <- ifelse(creditResults$obs == \"Bad\",\n",
    "                              \"True Outcome: Bad Credit\",\n",
    "                              \"True Outcome: Good Credit\")\n",
    "\n",
    "### Plot the probability of bad credit\n",
    "\n",
    "histogram(~prob|Label,\n",
    "          data = creditResults,\n",
    "          layout = c(2, 1),\n",
    "          nint = 20,\n",
    "          xlab = \"Probability of Bad Credit\",\n",
    "          type = \"count\")\n",
    "\n",
    "### Calculate and plot the calibration curve\n",
    "creditCalib <- calibration(obs ~ prob, data = creditResults)\n",
    "\n",
    "xyplot(creditCalib)\n",
    "\n",
    "### Create the confusion matrix from the test set.\n",
    "\n",
    "confusionMatrix(data = creditResults$pred,\n",
    "                reference = creditResults$obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### ROC curves:\n",
    "\n",
    "### Like glm(), roc() treats the last level of the factor as the event\n",
    "### of interest so we use relevel() to change the observed class data\n",
    "\n",
    "library(pROC)\n",
    "creditROC <- roc(relevel(creditResults$obs, \"Good\"), creditResults$prob)\n",
    "\n",
    "coords(creditROC, \"all\")[,1:3]\n",
    "\n",
    "auc(creditROC)\n",
    "\n",
    "ci.auc(creditROC)\n",
    "\n",
    "\n",
    "### Note the x-axis is reversed\n",
    "plot(creditROC)\n",
    "\n",
    "### Old-school:\n",
    "plot(creditROC, legacy.axes = TRUE)\n",
    "\n",
    "### Lift charts\n",
    "\n",
    "creditLift <- lift(obs ~ prob, data = creditResults)\n",
    "\n",
    "xyplot(creditLift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "summary(GermanCredit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "showChapterScript(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "showChapterOutput(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## runChapterScript(12)\n",
    "\n",
    "##        user     system    elapsed \n",
    "##  376332.996   8337.928  35694.682 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "showChapterScript(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "showChapterOutput(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##  runChapterScript(13)\n",
    "\n",
    "##       user    system   elapsed \n",
    "##  313451.24   2270.67  52861.72 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "showChapterScript(14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "showChapterOutput(14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## runChapterScript(14)\n",
    "\n",
    "##        user     system    elapsed \n",
    "##  208496.296    776.829 209791.456 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "showChapterScript(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "showChapterOutput(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## runChapterScript(16)\n",
    "\n",
    "##        user     system    elapsed \n",
    "##  243437.520    682.066 244138.032 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "showChapterScript(17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "showChapterOutput(17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## runChapterScript(17)\n",
    "\n",
    "##       user    system   elapsed \n",
    "##  492217.97  31824.96  39801.06 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "showChapterScript(18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "showChapterOutput(18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "runChapterScript(18)\n",
    "\n",
    "##     user  system elapsed \n",
    "##   78.161   0.635  79.081 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Section 18.1 Numeric Outcomes\n",
    "\n",
    "## Load the solubility data\n",
    "\n",
    "library(AppliedPredictiveModeling)\n",
    "data(solubility)\n",
    "\n",
    "trainData <- solTrainXtrans\n",
    "trainData$y <- solTrainY\n",
    "\n",
    "\n",
    "## keep the continuous predictors and append the outcome to the data frame\n",
    "SolContPred <- solTrainXtrans[, !grepl(\"FP\", names(solTrainXtrans))]\n",
    "numSolPred <- ncol(SolContPred)\n",
    "SolContPred$Sol <- solTrainY\n",
    "\n",
    "## Get the LOESS smoother and the summary measure\n",
    "library(caret)\n",
    "smoother <- filterVarImp(x = SolContPred[, -ncol(SolContPred)],\n",
    "                         y = solTrainY,\n",
    "                         nonpara = TRUE)\n",
    "smoother$Predictor <- rownames(smoother)\n",
    "names(smoother)[1] <- \"Smoother\"\n",
    "\n",
    "## Calculate the correlation matrices and keep the columns with the correlations\n",
    "## between the predictors and the outcome\n",
    "\n",
    "correlations <- cor(SolContPred)[-(numSolPred+1),(numSolPred+1)]\n",
    "rankCorrelations <- cor(SolContPred, method = \"spearman\")[-(numSolPred+1),(numSolPred+1)]\n",
    "corrs <- data.frame(Predictor = names(SolContPred)[1:numSolPred],\n",
    "                    Correlation = correlations,\n",
    "                    RankCorrelation  = rankCorrelations)\n",
    "\n",
    "## The maximal information coefficient (MIC) values can be obtained from the\n",
    "### minerva package:\n",
    "if (!(is.element(\"minerva\", installed.packages())))\n",
    "    install.packages(\"minerva\", repos=\"http://cran.us.r-project.org\")\n",
    "library(minerva)\n",
    "MIC <- mine(x = SolContPred[, 1:numSolPred], y = solTrainY)$MIC\n",
    "MIC <- data.frame(Predictor = rownames(MIC),\n",
    "                  MIC = MIC[,1])\n",
    "\n",
    "\n",
    "## The Relief values for regression can be computed using the CORElearn\n",
    "## package:\n",
    "\n",
    "library(CORElearn)\n",
    "ReliefF <- attrEval(Sol ~ .,  data = SolContPred,\n",
    "                    estimator = \"RReliefFequalK\")\n",
    "ReliefF <- data.frame(Predictor = names(ReliefF),\n",
    "                  Relief = ReliefF)\n",
    "\n",
    "## Combine them all together for a plot\n",
    "contDescrScores <- merge(smoother, corrs)\n",
    "contDescrScores <- merge(contDescrScores, MIC)\n",
    "contDescrScores <- merge(contDescrScores, ReliefF)\n",
    "\n",
    "rownames(contDescrScores) <- contDescrScores$Predictor\n",
    "\n",
    "contDescrScores\n",
    "\n",
    "contDescrSplomData <- contDescrScores\n",
    "contDescrSplomData$Correlation <- abs(contDescrSplomData$Correlation)\n",
    "contDescrSplomData$RankCorrelation <- abs(contDescrSplomData$RankCorrelation)\n",
    "contDescrSplomData$Group <- \"Other\"\n",
    "contDescrSplomData$Group[grepl(\"Surface\", contDescrSplomData$Predictor)] <- \"SA\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "featurePlot(solTrainXtrans[, c(\"NumCarbon\", \"SurfaceArea2\")],\n",
    "            solTrainY,\n",
    "            between = list(x = 1),\n",
    "            type = c(\"g\", \"p\", \"smooth\"),\n",
    "            df = 3,\n",
    "            aspect = 1,\n",
    "            labels = c(\"\", \"Solubility\"))\n",
    "\n",
    "splom(~contDescrSplomData[,c(3, 4, 2, 5)],\n",
    "      groups = contDescrSplomData$Group,\n",
    "      varnames = c(\"Correlation\", \"Rank\\nCorrelation\", \"LOESS\", \"MIC\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Now look at the categorical (i.e. binary) predictors\n",
    "SolCatPred <- solTrainXtrans[, grepl(\"FP\", names(solTrainXtrans))]\n",
    "SolCatPred$Sol <- solTrainY\n",
    "numSolCatPred <- ncol(SolCatPred) - 1\n",
    "\n",
    "tests <- apply(SolCatPred[, 1:numSolCatPred], 2,\n",
    "                  function(x, y)\n",
    "                    {\n",
    "                    tStats <- t.test(y ~ x)[c(\"statistic\", \"p.value\", \"estimate\")]\n",
    "                    unlist(tStats)\n",
    "                    },\n",
    "               y = solTrainY)\n",
    "\n",
    "## The results are a matrix with predictors in columns. We reverse this\n",
    "tests <- as.data.frame(t(tests))\n",
    "names(tests) <- c(\"t.Statistic\", \"t.test_p.value\", \"mean0\", \"mean1\")\n",
    "tests$difference <- tests$mean1 - tests$mean0\n",
    "\n",
    "tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Create a volcano plot\n",
    "xyplot(-log10(t.test_p.value) ~ difference,\n",
    "       data = tests,\n",
    "       xlab = \"Mean With Structure - Mean Without Structure\",\n",
    "       ylab = \"-log(p-Value)\",\n",
    "       type = \"p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Section 18.2 Categorical Outcomes\n",
    "\n",
    "## Load the segmentation data\n",
    "\n",
    "data(segmentationData)\n",
    "segTrain <- subset(segmentationData, Case == \"Train\")\n",
    "segTrain$Case <- segTrain$Cell <- NULL\n",
    "\n",
    "segTest <- subset(segmentationData, Case != \"Train\")\n",
    "segTest$Case <- segTest$Cell <- NULL\n",
    "\n",
    "## Compute the areas under the ROC curve\n",
    "aucVals <- filterVarImp(x = segTrain[, -1], y = segTrain$Class)\n",
    "aucVals$Predictor <- rownames(aucVals)\n",
    "\n",
    "## Cacluate the t-tests as before but with x and y switched\n",
    "segTests <- apply(segTrain[, -1], 2,\n",
    "                  function(x, y)\n",
    "                    {\n",
    "                    tStats <- t.test(x ~ y)[c(\"statistic\", \"p.value\", \"estimate\")]\n",
    "                    unlist(tStats)\n",
    "                    },\n",
    "               y = segTrain$Class)\n",
    "segTests <- as.data.frame(t(segTests))\n",
    "names(segTests) <- c(\"t.Statistic\", \"t.test_p.value\", \"mean0\", \"mean1\")\n",
    "segTests$Predictor <- rownames(segTests)\n",
    "\n",
    "## Fit a random forest model and get the importance scores\n",
    "library(randomForest)\n",
    "set.seed(791)\n",
    "rfImp <- randomForest(Class ~ ., data = segTrain,\n",
    "                      ntree = 2000,\n",
    "                      importance = TRUE)\n",
    "rfValues <- data.frame(RF = importance(rfImp)[, \"MeanDecreaseGini\"],\n",
    "                       Predictor = rownames(importance(rfImp)))\n",
    "\n",
    "## Now compute the Relief scores\n",
    "set.seed(791)\n",
    "\n",
    "ReliefValues <- attrEval(Class ~ ., data = segTrain,\n",
    "                         estimator=\"ReliefFequalK\", ReliefIterations = 50)\n",
    "ReliefValues <- data.frame(Relief = ReliefValues,\n",
    "                           Predictor = names(ReliefValues))\n",
    "\n",
    "## and the MIC statistics\n",
    "set.seed(791)\n",
    "segMIC <- mine(x = segTrain[, -1],\n",
    "               ## Pass the outcome as 0/1\n",
    "               y = ifelse(segTrain$Class == \"PS\", 1, 0))$MIC\n",
    "segMIC <- data.frame(Predictor = rownames(segMIC),\n",
    "                  MIC = segMIC[,1])\n",
    "\n",
    "\n",
    "rankings <- merge(segMIC, ReliefValues)\n",
    "rankings <- merge(rankings, rfValues)\n",
    "rankings <- merge(rankings, segTests)\n",
    "rankings <- merge(rankings, aucVals)\n",
    "rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rankings$channel <- \"Channel 1\"\n",
    "rankings$channel[grepl(\"Ch2$\", rankings$Predictor)] <- \"Channel 2\"\n",
    "rankings$channel[grepl(\"Ch3$\", rankings$Predictor)] <- \"Channel 3\"\n",
    "rankings$channel[grepl(\"Ch4$\", rankings$Predictor)] <- \"Channel 4\"\n",
    "rankings$t.Statistic <- abs(rankings$t.Statistic)\n",
    "\n",
    "splom(~rankings[, c(\"PS\", \"t.Statistic\", \"RF\", \"Relief\", \"MIC\")],\n",
    "      groups = rankings$channel,\n",
    "      varnames = c(\"ROC\\nAUC\", \"Abs\\nt-Stat\", \"Random\\nForest\", \"Relief\", \"MIC\"),\n",
    "      auto.key = list(columns = 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Load the grant data. A script to create and save these data is contained\n",
    "## in the same directory as this file.\n",
    "\n",
    "source( file.path( scriptLocation(), \"CreateGrantData.R\" ),  echo=TRUE )\n",
    "\n",
    "load(\"grantData.RData\")\n",
    "\n",
    "dataSubset <- training[pre2008, c(\"Sponsor62B\", \"ContractValueBandUnk\", \"RFCD240302\")]\n",
    "\n",
    "## This is a simple function to compute several statistics for binary predictors\n",
    "tableCalcs <- function(x, y)\n",
    "  {\n",
    "  tab <- table(x, y)\n",
    "  fet <- fisher.test(tab)\n",
    "  out <- c(OR = fet$estimate,\n",
    "           P = fet$p.value,\n",
    "           Gain = attrEval(y ~ x, estimator = \"GainRatio\"))\n",
    "  }\n",
    "\n",
    "## lapply() is used to execute the function on each column\n",
    "tableResults <- lapply(dataSubset, tableCalcs, y = training[pre2008, \"Class\"])\n",
    "\n",
    "## The results come back as a list of vectors, and \"rbind\" is used to join\n",
    "## then together as rows of a table\n",
    "tableResults <- do.call(\"rbind\", tableResults)\n",
    "tableResults\n",
    "\n",
    "## The permuted Relief scores can be computed using a function from the\n",
    "## AppliedPredictiveModeling package.\n",
    "\n",
    "permuted <- permuteRelief(x = training[pre2008, c(\"Sponsor62B\", \"Day\", \"NumCI\")],\n",
    "                          y = training[pre2008, \"Class\"],\n",
    "                          nperm = 500,\n",
    "                          ### the remaining options are passed to attrEval()\n",
    "                          estimator=\"ReliefFequalK\",\n",
    "                          ReliefIterations= 50)\n",
    "\n",
    "## The original Relief scores:\n",
    "permuted$observed\n",
    "\n",
    "## The number of standard deviations away from the permuted mean:\n",
    "permuted$standardized\n",
    "\n",
    "## The distributions of the scores if there were no relationship between the\n",
    "## predictors and outcomes\n",
    "\n",
    "histogram(~value|Predictor,\n",
    "          data = permuted$permutations,\n",
    "          xlim = extendrange(permuted$permutations$value),\n",
    "          xlab = \"Relief Score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "showChapterScript(19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "showChapterOutput(19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## runChapterScript(19)\n",
    "\n",
    "##       user     system    elapsed \n",
    "## 257587.585   7078.267  35323.717 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

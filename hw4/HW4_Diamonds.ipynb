{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: lattice\n",
      "\n",
      "Attaching package: ‘DAAG’\n",
      "\n",
      "The following object is masked from ‘package:MASS’:\n",
      "\n",
      "    hills\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#HW4\n",
    "\n",
    "not.installed <- function(pkg) !is.element(pkg, installed.packages()[,1])\n",
    "\n",
    "if (not.installed(\"ggplot2\")) install.packages(\"ggplot2\")\n",
    "if (not.installed(\"devtools\")) install.packages(\"devtools\")\n",
    "if (not.installed(\"DAAG\")) install.packages(\"DAAG\")\n",
    "if (not.installed(\"InformationValue\")) install.packages(\"InformationValue\") \n",
    "\n",
    "library(ggplot2)\n",
    "library(MASS)\n",
    "library(DAAG)  \n",
    "library(lattice)\n",
    "library(InformationValue) \n",
    "    \n",
    "#library(devtools)\n",
    "#install_github(\"ggbiplot\", \"vqv\")\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "diamonds = subset( diamonds, (x>0) & (y>0) & (z>0) )  #  There are actually some zero values, we omit them.\n",
    "numeric.diamonds = transform( diamonds,\n",
    "                              cut = as.numeric(unclass(diamonds$cut)),\n",
    "                              color = as.numeric(unclass(diamonds$color)),\n",
    "                              clarity = as.numeric(unclass(diamonds$clarity))\n",
    "                   ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in nrow(numeric.diamonds): object 'numeric.diamonds' not found\n",
     "output_type": "error",
     "traceback": [
      "Error in nrow(numeric.diamonds): object 'numeric.diamonds' not found\nTraceback:\n",
      "1. nrow(numeric.diamonds)"
     ]
    }
   ],
   "source": [
    "MY_UID = 123456789 ########## you must enter your UCLA UID here !!!\n",
    "\n",
    "set.seed( MY_UID )\n",
    "\n",
    "n = nrow( numeric.diamonds )\n",
    "sample.size = 0.75 * n   ###### Use 75% of the data for the training set\n",
    "training.row.ids = sample( (1:n), sample.size )\n",
    "\n",
    "my.training.set = numeric.diamonds[  training.row.ids, ]\n",
    "my.test.set     = numeric.diamonds[ -training.row.ids, ]   # set complement of training.set.ids\n",
    "\n",
    "head(numeric.diamonds)\n",
    "head(my.training.set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classification_accuracy = function( model, test.data, test.solutions ) {\n",
    "    predictions = predict(model, test.data)\n",
    "    incorrect.predictions  =  (predictions$class != test.solutions)\n",
    "    incorrect.ids = test.solutions[incorrect.predictions]\n",
    "    confusion.matrix = table( test.solutions, predictions$class )\n",
    "    \n",
    "    accuracy = (length(test.solutions) - length(incorrect.ids)) / length(test.solutions)\n",
    "    return (accuracy)\n",
    "\n",
    "}\n",
    "\n",
    "linear_regression_accuracy = function( model, test.data, test.solutions ) {\n",
    "       \n",
    "    predictions = predict(model, test.data)    \n",
    "    actuals_preds = data.frame(cbind(actuals=test.solutions, predicteds=predictions))  # make actuals_predicteds dataframe.  \n",
    "    correlation_accuracy = cor(actuals_preds)  # 82.7%   \n",
    "    min_max_accuracy = mean(apply(actuals_preds, 1, min) / apply(actuals_preds, 1, max))    \n",
    "    mape = mean(abs((actuals_preds$predicteds - actuals_preds$actuals))/actuals_preds$actuals)          \n",
    "    \n",
    "#     #k-validation graphs\n",
    "#     cvResults = suppressWarnings(cv.lm(numeric.diamonds, model, m=5, dots=FALSE, seed=MY_UID, legend.pos=\"topleft\",  printit=FALSE, main=\"K-folds Validation\")) \n",
    " \n",
    "#     cat('AIC (lower the better): ', AIC(model), '\\n')\n",
    "#     cat('BIC (lower the better): ', BIC(model), '\\n')\n",
    "    \n",
    "#     cat('min_max_accuracy (higher the better): ', min_max_accuracy, '\\n')  \n",
    "#     cat('mape (lower the better): ', mape, '\\n')   \n",
    "#     print(summary(model))\n",
    "    \n",
    "#     cat('mean squared error (lower the better): ',attr(cvResults, 'ms'), '\\n')\n",
    "   \n",
    "    rsquared = summary(model)$r.squared\n",
    "    return(rsquared)\n",
    "}\n",
    "\n",
    "logistic_regression_accuracy = function( model, test.data, test.solutions ) {\n",
    "    #because glm returns 0 or 1, we can compare it against test data to calculate accuracy.\n",
    "    \n",
    "    Predictions = predict( model, newdata = test.data, type = \"response\")\n",
    "    PredictionClasses = round(Predictions) # Turn the numeric values into {0,1} values\n",
    "    \n",
    "    PredictionsIncorrect = (PredictionClasses != test.solutions)\n",
    "    \n",
    "    sum(PredictionsIncorrect) # number of incorrect predictions\n",
    "\n",
    "    \n",
    "    (ConfusionMatrix = table( PredictionClasses, test.solutions ) )\n",
    "    \n",
    "    #print(ConfusionMatrix)\n",
    "    \n",
    "    accuracy = sum(diag(ConfusionMatrix)) / nrow(test.data)\n",
    "    \n",
    "    #accuracy = (length(test.solutions) - length(incorrect.ids)) / length(test.solutions)\n",
    "    \n",
    "    return (accuracy)\n",
    "    \n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#base line models\n",
    "baseline_m1 = qda( cut ~ .,           data=my.training.set )\n",
    "baseline_m1_accuracy = classification_accuracy(baseline_m1, my.test.set, my.test.set$cut)\n",
    "\n",
    "baseline_m2 = lm(  price ~ .,         data=my.training.set )\n",
    "baseline_m2_accuracy = linear_regression_accuracy(baseline_m2, my.test.set, my.test.set$price)\n",
    "\n",
    "baseline_m3 = lm(  log10(price) ~ .,  data=my.training.set )\n",
    "baseline_m3_accuracy = linear_regression_accuracy(baseline_m3, my.test.set, my.test.set$price)\n",
    "\n",
    "baseline_m4 = suppressWarnings( glm( I(price>1500) ~ ., data=my.training.set, family=binomial ))\n",
    "baseline_m4_accuracy = logistic_regression_accuracy( baseline_m4, my.test.set, I(my.test.set$price>1500) ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#improved models\n",
    "m1 = qda(cut ~ price + carat + depth + table, data=my.training.set)\n",
    "m1_accuracy = classification_accuracy(m1, my.test.set, my.test.set$cut)\n",
    "\n",
    "m2 = lm(price ~ (carat + cut + color + clarity + depth + table)^2 + x + y + z, data=my.training.set )\n",
    "m2_accuracy = linear_regression_accuracy(m2, my.test.set, my.test.set$price)\n",
    "\n",
    "m3 = lm(log10(price) ~ (carat + cut + color + clarity + depth + table)^2 + log10(x) + log10(y) + log10(z), data=my.training.set)\n",
    "m3_accuracy = linear_regression_accuracy(m3, my.test.set, my.test.set$price)\n",
    "\n",
    "m4 = suppressWarnings( glm( I(price>1500) ~ (carat + cut + color + clarity + depth + table)^2, data=my.training.set, family=binomial ))\n",
    "m4_accuracy = logistic_regression_accuracy( m4, my.test.set, I(my.test.set$price>1500) ) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.629822 , qda( cut ~ .,           data=my.training.set ) \n",
      "0.9042087 , lm(  price ~ .,         data=my.training.set ) \n",
      "6.068938e-09 , lm(  log10(price) ~ .,  data=my.training.set ) \n",
      "0.9781899 , glm( I(price>1500) ~ ., data=my.training.set, family=binomial ) \n",
      "0.6422107 , qda( cut ~ price + table + color + clarity, data=my.training.set ) \n",
      "0.9571075 , lm( price ~ (carat + cut + color + clarity + depth + table)^2 + x + y + z, data=my.training.set ) \n",
      "6.091187e-09 , lm(log10(price) ~ (carat + cut + color + clarity + depth + table)^2 + log10(x) + log10(y) + log10(z), data=my.training.set) \n",
      "0.9869436 , glm( I(price>1500) ~ (carat + cut + color + clarity + depth + table)^2, data=my.training.set, family=binomial ) \n"
     ]
    }
   ],
   "source": [
    "cat(baseline_m1_accuracy,', qda( cut ~ .,           data=my.training.set )', '\\n')\n",
    "cat(baseline_m2_accuracy,', lm(  price ~ .,         data=my.training.set )', '\\n')\n",
    "cat(baseline_m3_accuracy,', lm(  log10(price) ~ .,  data=my.training.set )', '\\n')\n",
    "cat(baseline_m4_accuracy,', glm( I(price>1500) ~ ., data=my.training.set, family=binomial )', '\\n')\n",
    "\n",
    "cat(m1_accuracy,', qda( cut ~ price + table + color + clarity, data=my.training.set )', '\\n')\n",
    "cat(m2_accuracy,', lm( price ~ (carat + cut + color + clarity + depth + table)^2 + x + y + z, data=my.training.set )', '\\n')\n",
    "cat(m3_accuracy,', lm(log10(price) ~ (carat + cut + color + clarity + depth + table)^2 + log10(x) + log10(y) + log10(z), data=my.training.set)', '\\n')\n",
    "cat(m4_accuracy,', glm( I(price>1500) ~ (carat + cut + color + clarity + depth + table)^2, data=my.training.set, family=binomial )', '\\n')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
